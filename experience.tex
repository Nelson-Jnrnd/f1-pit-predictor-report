Au cours de ce travail nous utilisons un apprentissage supervisé, l'idée est d'avoir une approche d'imitation learning.
C'est à dire que les données historiques sont utilisées pour l'entrainement et les décisions prises par les équipes sont considérées comme ground-truth.
On utilise cette méthode car elle nécessite moins de mise en place qu'une approche par renforcement qui nécessiterais la mise en place d'une simulation
pour estimer le temps gagné dans les différents scénarios hypothètiques.
En revanche l'inconvénient de cette méthode est que les modèles ne peuvent pas surpasser les performances des stratégistes étant donnée qu'ils tentent de les copier.
Il est estimé que cela n'est pas un problème dans la mesure où les stratégies employées sont bonnes en moyenne.

\section{Préparation des données}
Les modèles sont entrainés sur les années 2019 à 2022, en réservant l'année 2023 comme set de test afin d'évaluer la capacité du modèle à généraliser
sur des données qui ne sont pas inclus dans le set d'entrainement.
Étant donné la quantité importante de données, nous avons choisi d'effectuer une division du dataset en un jeu d'entraînement et un jeu de validation avec un ratio de 0,2.

Les étapes de preprocessing ont ensuite été appliquée sur les sets de données individuellement afin d'éviter le data leakage.
Ce phénomène se produit lorsque des informations provenant du set de test ou de validation sont incorporées dans le processus d'entraînement du modèle,
ce qui fausse les résultats de l'évaluation et conduit à une estimation optimiste des performances du modèle.

\section{Modélisation}
L'objectif étant de déterminer la stratégie d'une voiture, le problème comporte 2 parties :
\begin{itemize}
    \item La première est de déterminer si la voiture s'arrête ou non.
    \item La seconde est de déterminer les pneus à utiliser si la voiture s'arrête.
\end{itemize}
On peut donc modéliser le problème comme une classification binaire suivie d'une classification multi-classes,
où la première détermine si la voiture s'arrête ou non et la seconde détermine les pneus à utiliser si la voiture s'arrête.
Une autre modélisation possible est de considérer les 2 parties comme un problème de classification multi-classes,
où le modèle détermine directement les pneus à utiliser et la classe "ne s'arrête pas" est considérée comme une classe à part entière.
Une dernière modélisation possible est de considérer les 2 parties comme 2 sorties indépendantes d'un même modèle si l'algorithme utilisé le permet.
Les 3 modélisations sont illustrées par la figure \ref{modelisations}.
%TODO : ajouter un schéma pour chaque modélisation

Les modélisations basées sur des modèles de régression sont aussi possibles mais elles ne sont pas considérées car selon l'étude de l'état de l'art
\cite{app10217805} les modèles de classification sont plus performants.

Dans les prochaines sections nous allons détailler les modélisations explorées et les résultats obtenus.
\section{Classification binaire pour la décision d'arrêt}
\subsection{Random Forest}
%La première modélisation explorée est celle d'une classification binaire
%où le modèle considère les informations d'un seul tour de course en entrée et détermine en sortie si la voiture s'arrête ou non à la fin de celui-ci.
Le premier algorithme de classification utilisé est Random Forest, c'est un algorithme d'ensemble qui combine plusieurs arbres de décision.
Il a été choisit car il est performant sur des sets de données déséquilibrés avec mécanisme de pondération des classes et qu'il est interprétable.
L'interprétabilité est un critère important car il permet de comprendre les décisions prises par le modèle se qui vas dans un premier temps nous aider à l'améliorer
et dans un second temps à donner confiance aux utilisateurs du modèle.

L'implémentation utilisée est celle de scikit-learn \cite{scikitLearnRandomForest} qui est une librairie open-source de machine learning pour python.
Les hyperparamètres du modèle ont été choisis après une recherche par grid search avec une validation croisée sur 4 folds.

Grid search est une méthode d'optimisation des hyperparamètres dans laquelle un modèle est entrainé pour chaque combinaison d'hyperparamètres.
Il est donc important de définir une métrique pour estimer la qualité des modèles entrainés.
\subsubsection{Métriques}
Étant donnés, notre set de données très déséquilibré, nous n'allons pas utiliser l'accuracy ou le score ROC AUC car ils ne sont pas adaptés.
Le choix repose entre le F1-score et la balanced accuracy qui sont les 2 métriques les plus communement utilisées pour les sets de données déséquilibrées.

Pour décider quelle métrique favoriser on s'attarde sur leur définition respective à partir des ensembles des classes prédites.
La figure \ref{confusion_diagram} visualise graphiquement ces ensembles pour le cas d'une classification binaire.
\fig[H, width=0.5\textwidth]{\label{confusion_diagram}Visualisation des résultats d'un système de classification binaire}{confusion_matrice.drawio.svg}

La balanced accuracy est définie comme la moyenne entre la sensitivité (rappel) et la spécificité :
$$
    \frac{\text{spécificité}+\text{sensitivité}}{2}
$$
La figure \ref{sensitivity_specificity} visualise les ensembles considérés par la spécificité et la sensitivité.
\fig[H, width=0.4\textwidth]{\label{sensitivity_specificity}Visualisation de la spécificité et de la sensitivité}{sensitivity_specificity.drawio.svg}
la sensitivité ou rappel indique combien de fois le modèle a correctement prédit la classe positive parmi tous les échantillons positifs réels.
La spécificité est l'équivalent de la sensitivité pour la classe négative, elle mesure la proportion d'échantillons négatifs correctement prédits par le modèle.

Le F1-score est défini comme la moyenne harmonique entre la précision et la sensitivité :
$$
    F=2\cdot\frac{\text{précision} \cdot \text{sensitivité}}{\text{précision} + \text{sensitivité}}
$$
La précision indique combien de fois le modèle a correctement prédit la classe positive parmi toutes les prédictions positives faites,
La figure \ref{precision_recall} montre les ensembles considérés par ses 2 métriques.
\fig[H, width=0.4\textwidth]{\label{precision_recall}Visualisation de la précision et de la sensitivité}{precision_recall.drawio.svg}
On peut observer que le F-score pénalise une précision ou un rappel faible, 2 métriques qui ne considèrent pas les vrai négatifs et qui accordent une grande importance à l'identification de la classe positive.
Pour cette raison, le f1 score est souvant utilisé dans les problèmes de détections d'anomalies, car les sets de données sont fortement biasés en faveur de la classe négative.
Notre cas est assez similaire à ce cas de figure avec 97\% de cas négatifs, le f1 score devrait donc être plus adapté.

En utilisant l'implémentation GridSearchCV de scikit-learn, nous avons effectué une optimisation du f1 score, les hyperparamètres sont en table \ref{grid_params}.

\begin{table}[H]
    \begin{center}
        \caption{\label{grid_params}Paramètres de la recherche GridSearchCV}
        \begin{tabular}{r|l}
            hyperparamètre & valeurs                            \\ \hline
            class weight   & None, balanced, balanced subsample \\
            n estimator    & 100, 500, 1000, 2000               \\
            max depth      & 5, 20, None                        \\
            max features   & sqrt, log2, None                   \\
        \end{tabular}
    \end{center}
\end{table}

La table complète des résultats est en annexe \ref{grid_search_results}.

Le meilleur modèle obtient un f1 score de 0.13 avec les paramètres en table \ref{rf_params}.
Ce score est très faible, nous allons étudier les performances du modèle sur le set de validation pour comprendre ce résultat.

\begin{table}[H]
    \begin{center}
        \caption{\label{rf_params}Paramètres du meilleur modèle}
        \begin{tabular}{r|l}
            hyperparamètre & valeur             \\ \hline
            class weight   & balanced subsample \\
            n estimator    & 2000               \\
            max depth      & 5                  \\
            max features   & sqrt               \\
        \end{tabular}
    \end{center}
\end{table}

Les métriques des résultats sont observables en table \ref{rf_results} et la matrice de confusion en table \ref{rf_matrix}.
En raison de la nature déséquilibrée de notre classification, on ne peut pas se fier à la métrique d'accuracy.

\begin{table}[H]
    \begin{center}
        \caption{\label{rf_matrix}Matrice de confusion de l'entrainement du premier modèle, les colonnes représentent les prédictions et les lignes le ground truth.}
        \begin{tabular}{r|cc}
                    & Négatif & Positif \\ \hline
            Négatif & 10470   & 3422    \\
            Positif & 103     & 281     \\
        \end{tabular}
    \end{center}
\end{table}

\begin{table}[H]
    \begin{center}
        \caption{\label{rf_results}Matrice de confusion de l'entrainement du premier modèle, les colonnes représentent les prédictions et les lignes le ground truth.}
        \begin{tabular}{r|ccc}
            Classe  & Precision & Recall & F1Score \\ \hline
            Négatif & 0.99      & 0.75   & 0.86    \\
            Positif & 0.08      & 0.73   & 0.14    \\
        \end{tabular}
    \end{center}
\end{table}

On remarque dans les résultats que le modèle à de la peine à classifier les cas positifs avec beaucoup de faux-positifs.

\subsubsection{Analyse des résultats}
Premièrement, pour confirmer que le modèle utilise correctement les variables pour la décision nous étudions les raisons derrière ses décisions avec des méthodes d'explications.

Random Forest étant un algorithme interprétable, il nous permet de mesurer en figure \ref{feature_importance_rf_pit} l'importance des features dans sa classification.
L'importance d'une feature est calculée comme la réduction totale de l'entropie normalisée apportée par cette feature.
\fig[H, width=0.9\textwidth]{
    \label{feature_importance_rf_pit}Importance des features du modèle de Random Forest pour la classification des arrêts au stand.
}{feature_importance_rf_pit.svg}

Les features TyreLife, LapNumber et Stint sont les plus importantes. Ce qui est cohérent avec nos connaissances sur la stratégie de course.
Les features les moins utilisées sont celles liées au circuit, nous supposons que c'est parce que la feature est one-hot encoded se qui crée une grande matrice creuse
les features individuelles apporte donc peu d'information au modèle.

Pour comprendre les résultats, nous allons analyser les prédictions du modèle sur le set de validation.
Random Forest est un algorithme interprétable localement de lui même mais le grand nombre d'arbre dans notre modèle rend cela plus difficile.
Nous utilisons LIME (Local Interpretable Model-agnostic Explanations) \cite{lime} un outil d'explication locale,
c'est à dire qu'il permet d'expliquer les prédictions pour une instance donnée.
Pour ce faire LIME fonctionne de la manière suivante:
\begin{enumerate}
    \item On génère un ensemble de données autour de l'instance à expliquer en perturbant légerement les features.
    \item On prédit la classe de l'ensemble de données généré avec le modèle.
    \item On entraine un modèle explicable avec comme features l'ensemble de données généré et comme labels les prédictions du modèle. LIME attribue un poids à chaque instance de l'ensemble de données généré en fonction de la distance avec l'instance à expliquer. Les instances les plus proches sont plus importantes car elles sont plus représentatives de l'instance que l'on cherche à expliquer.
\end{enumerate}

Nous allons étudier les décisions prises pendant la course de Lance Stroll au Grand Prix d'Espagne 2023,
on peut observer la progression  des probabilités d'arrêt au stand en figure \ref{stroll_barcelona_2023}.
\fig[H, width=0.9\textwidth]{
    \label{stroll_barcelona_2023}Progression des probabilités d'arrêt au stand pour le Grand Prix d'Espagne 2023 de Lance Stroll.
    Les probabilités sont calculées à chaque tour en utilisant les données du tour précédent.
    Les lignes verticales rouges indiquent la target du modèle, c'est à dire un tour avant l'arrêt au stand réel.
    Les bares horizontales en haut de la figure indique le composé de pneu utilisé (rouge = tendre, jaune = médium, gris = dur).
}{barcelona_2023_lance_stroll.svg}

Le modèle semble capturer la relation entre les arrêts au stand et les données de course.
Les probabilités d'arrêt au stand augmentent à chaque tour et diminuent drastitquement après un arrêt au stand réel.
Les probabilités augmentent également plus faiblement pendant le relai sur pneus durs se qui est cohérent avec la dégradation plus faible de ces pneus.
Cependant le modèle ne semble pas pouvoir augmenter la probabilité d'arrêt au stand assez rapidement pour éviter les faux positifs, cela explique les résultats de la matrice de confusion.
Ces résultats sont cohérents avec les observations de l'étude \cite{app10217805}.

Nous allons maintenant étudier les explications de LIME pour les tours 2, 8 et 16 de cette course.
Premièrement, on peut observer en figure \ref{lime_2} les résultats de LIME pour le tour 2, où le modèle à décidé de ne pas s'arrêter.
\fig[H, width=0.9\textwidth]{
    \label{lime_2}Résultat de l'algorithme d'explication LIME pour le tour 2 de la course de Lance Stroll à Barcelone en 2023.
}{explanation_barcelona_stroll_2.svg}

On peut voir que l'age des pneus faible est la raison principale de la décision. En figure \ref{lime_8} les résultats de LIME pour le tour 8, le premier tour où le modèle décide de s'arrêter.
\fig[H, width=0.9\textwidth]{
    \label{lime_8}Résultat de l'algorithme d'explication LIME pour le tour 8 de la course de Lance Stroll à Barcelone en 2023.
}{explanation_barcelona_stroll_8.svg}

L'age des pneus a augmenté et sont impact négatif sur la décision est plus faible. L'absence de pneumatique dur et le fait que la voiture soit dans son
premier relai sont les raisons principales de la décision. En figure \ref{lime_16} les résultats de LIME pour le tour 16, le premier tour après l'arrêt au stand.
\fig[H, width=0.9\textwidth]{
    \label{lime_16}Résultat de l'algorithme d'explication LIME pour le tour 16 de la course de Lance Stroll à Barcelone en 2023.
}{explanation_barcelona_stroll_16.svg}
La prédiction est redevenue négative car l'age des pneus est de nouveau faible. De plus, on peut observer l'impact négatif sur la prédiction du second relai.
Le modèle a appris que les voitures font au moins deux relais il est donc normal que le second relai ait un impact négatif sur la prédiction.
Dans l'ensemble, les features utilisées par le modèle sont cohérentes avec nos connaissances sur la stratégie de course.

Pour étudier le comportement du modèle face au périodes de safety car, on observe la progression des probabilités d'arrêt au stand pour le Grand Prix d'Azerbaïdjan 2023 de Sergio Perez en figure \ref{baku_2023}.

\fig[H, width=0.9\textwidth]{
    \label{baku_2023}Progression des probabilités d'arrêt au stand pour le Grand Prix d'Azerbaïdjan 2023 de Sergio Perez.
    Les zones jaunes indiquent les périodes de drapeau jaune, de safety car ou de virtual safety car.
}{baku_2023_sergio_perez.svg}

La période de drapeau jaune au tour 10 a poussé le modèle à prédire un arrêt au stand pour Sergio Perez.
Ce qui est cohérent avec la stratégie de course de l'équipe Red Bull qui a effectué un arrêt au stand pour Sergio Perez au tour 11.
En figure \ref{lime_baku_10} on peut observer les résultats de LIME pour le tour 10, le drapeau jaune est la raison principale de la décision.
\fig[H, width=0.9\textwidth]{
    \label{lime_baku_10}Résultat de l'algorithme d'explication LIME pour le tour 10 de la course de Sergio Perez à Bakou en 2023.
}{explanation_baku_10.svg}

Cependant la période de drapeau jaune au tour 49 a également poussé le modèle à prédire un arrêt au stand alors qu'il n'y en a pas eu.
Cela est dû à un manque d'information sur la raison du drapeau jaune. La première période de drapeau jaune est due à un accident et donc est fortement suseptible d'entrainer une voiture de sécurité.
Alors que la deuxième période de drapeau jaune est due à une sortie de piste sans conséquence et donc n'entraine pas de voiture de sécurité.
Ces information ne sont pas disponibles dans les données car elles sont observables uniquement en regardant les images de la course.
En figure \ref{lime_baku_49} on peut voir que le drapeau jaune est encore une fois la cause de la décision.
\fig[H, width=0.9\textwidth]{
    \label{lime_baku_49}Résultat de l'algorithme d'explication LIME pour le tour 49 de la course de Sergio Perez à Bakou en 2023.
}{explanation_baku_49.svg}

\subsection{Réseaux de neurones}


\subsection{Séries temporelles}

Les précédentes expériences utilisaient uniquement le dernier tour pour prédire la probabilité d'arrêt au stand.
Cependant, il sera plus intéressant d'utiliser les données de plusieurs tours car cela permettra de mieux capturer les tendances.

La formulation du problème devient alors une prédiction de séries temporelles.
On considère alors les données des tours comme une séquence.
Cette formulation est plus naturelle car les données sont une discrétisation par tour de données de capteurs qui sont des séries temporelles.
Elles sont enregistrées de façon séquentielle et sont donc naturellement ordonnées et corrélées.

Il faut cependant grouper les données par course et par voiture pour avoir des séquences de données cohérentes.
On peut observer en figure \ref{time_series_example} la nature séquentielle des données pour un sous ensemble de features.
