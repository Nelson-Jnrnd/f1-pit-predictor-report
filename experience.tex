Au cours de ce travail nous utilisons un apprentissage supervisé, l'idée est d'avoir une approche d'imitation learning.
C'est à dire que les données historiques sont utilisées pour l'entrainement et les décisions prises par les équipes sont considérées comme ground-truth.
On utilise cette méthode car elle nécessite moins de mise en place qu'une approche par renforcement qui nécessiterais la mise en place d'une simulation
pour estimer le temps gagné dans les différents scénarios hypothètiques.
En revanche l'inconvénient de cette méthode est que les modèles ne peuvent pas surpasser les performances des stratégistes étant donnée qu'ils tentent de les copier.
Il est estimé que cela n'est pas un problème dans la mesure où les stratégies employées sont bonnes en moyenne.

\section{Préparation des données}
Les modèles sont entrainés sur les années 2019 à 2022, en réservant l'année 2023 comme set de test afin d'évaluer la capacité du modèle à généraliser
sur des données qui ne sont pas inclus dans le set d'entrainement.
Étant donné la quantité importante de données, nous avons choisi d'effectuer une division du dataset en un jeu d'entraînement et un jeu de validation avec un ratio de 0,2.

Les étapes de preprocessing ont ensuite été appliquée sur les sets de données individuellement afin d'éviter le data leakage.
Ce phénomène se produit lorsque des informations provenant du set de test ou de validation sont incorporées dans le processus d'entraînement du modèle,
ce qui fausse les résultats de l'évaluation et conduit à une estimation optimiste des performances du modèle.

\section{Modélisation}
L'objectif étant de déterminer la stratégie d'une voiture, le problème comporte 2 parties :
\begin{itemize}
    \item La première est de déterminer si la voiture s'arrête ou non.
    \item La seconde est de déterminer les pneus à utiliser si la voiture s'arrête.
\end{itemize}
On peut donc modéliser le problème comme une classification binaire suivie d'une classification multi-classes,
où la première détermine si la voiture s'arrête ou non et la seconde détermine les pneus à utiliser si la voiture s'arrête.
Une autre modélisation possible est de considérer les 2 parties comme un seul problème de classification multi-classes,
où le modèle détermine directement les pneus à utiliser et la classe "ne s'arrête pas" est considérée comme une classe à part entière.
Une dernière modélisation possible est de considérer les 2 parties comme 2 sorties indépendantes d'un même modèle si l'algorithme utilisé le permet.
Les 3 modélisations sont illustrées par la figure \ref{modelisations}.
%TODO : ajouter un schéma pour chaque modélisation

Les modélisations basées sur des modèles de régression sont aussi possibles mais elles ne sont pas considérées car selon l'étude de l'état de l'art
\cite{app10217805} les modèles de classification sont plus performants.

Dans les prochaines sections nous allons détailler les modélisations explorées et les résultats obtenus.
\section{Classification binaire pour la décision d'arrêt}
\subsection{Random Forest}
%La première modélisation explorée est celle d'une classification binaire
%où le modèle considère les informations d'un seul tour de course en entrée et détermine en sortie si la voiture s'arrête ou non à la fin de celui-ci.
Le premier algorithme de classification utilisé est Random Forest, c'est un algorithme d'ensemble qui combine plusieurs arbres de décision.
Il a été choisit car il est performant sur des sets de données déséquilibrés avec mécanisme de pondération des classes et qu'il est interprétable.
L'interprétabilité est un critère important car il permet de comprendre les décisions prises par le modèle se qui vas dans un premier temps nous aider à l'améliorer
et dans un second temps à donner confiance aux utilisateurs du modèle.

L'implémentation utilisée est celle de scikit-learn \cite{scikitLearnRandomForest} qui est une librairie open-source de machine learning pour python.
Les hyperparamètres du modèle ont été choisis après une recherche par grid search avec une validation croisée sur 4 folds.

Grid search est une méthode d'optimisation des hyperparamètres dans laquelle un modèle est entrainé pour chaque combinaison d'hyperparamètres.
Il est donc important de définir une métrique pour estimer la qualité des modèles entrainés.
\subsubsection{Métriques}
Étant donnés, notre set de données très déséquilibré, nous n'allons pas utiliser l'accuracy ou le score ROC AUC car ils ne sont pas adaptés.
Le choix repose entre le F1-score et la balanced accuracy qui sont les 2 métriques les plus communement utilisées pour les sets de données déséquilibrées.

Pour décider quelle métrique favoriser on s'attarde sur leur définition respective à partir des ensembles des classes prédites.
La figure \ref{confusion_diagram} visualise graphiquement ces ensembles pour le cas d'une classification binaire.
\fig[H, width=0.5\textwidth]{\label{confusion_diagram}Visualisation des résultats d'un système de classification binaire}{confusion_matrice.drawio.svg}

La balanced accuracy est définie comme la moyenne entre la sensitivité (rappel) et la spécificité :
$$
    \frac{\text{spécificité}+\text{sensitivité}}{2}
$$
La figure \ref{sensitivity_specificity} visualise les ensembles considérés par la spécificité et la sensitivité.
\fig[H, width=0.4\textwidth]{\label{sensitivity_specificity}Visualisation de la spécificité et de la sensitivité}{sensitivity_specificity.drawio.svg}
la sensitivité ou rappel indique combien de fois le modèle a correctement prédit la classe positive parmi tous les échantillons positifs réels.
La spécificité est l'équivalent de la sensitivité pour la classe négative, elle mesure la proportion d'échantillons négatifs correctement prédits par le modèle.

Le F1-score est défini comme la moyenne harmonique entre la précision et la sensitivité :
$$
    F=2\cdot\frac{\text{précision} \cdot \text{sensitivité}}{\text{précision} + \text{sensitivité}}
$$
La précision indique combien de fois le modèle a correctement prédit la classe positive parmi toutes les prédictions positives faites,
La figure \ref{precision_recall} montre les ensembles considérés par ses 2 métriques.
\fig[H, width=0.4\textwidth]{\label{precision_recall}Visualisation de la précision et de la sensitivité}{precision_recall.drawio.svg}
On peut observer que le F-score pénalise une précision ou un rappel faible, 2 métriques qui ne considèrent pas les vrai négatifs et qui accordent une grande importance à l'identification de la classe positive.
Pour cette raison, le f1 score est souvant utilisé dans les problèmes de détections d'anomalies, car les sets de données sont fortement biasés en faveur de la classe négative.
Notre cas est assez similaire à ce cas de figure avec 97\% de cas négatifs, le f1 score devrait donc être plus adapté.

En utilisant l'implémentation GridSearchCV de scikit-learn, nous avons effectué une optimisation du f1 score, les hyperparamètres sont en table \ref{grid_params}.

\begin{table}[H]
    \begin{center}
        \caption{\label{grid_params}Paramètres de la recherche GridSearchCV}
        \begin{tabular}{r|l}
            hyperparamètre & valeurs                            \\ \hline
            class weight   & None, balanced, balanced subsample \\
            n estimator    & 100, 500, 1000, 2000               \\
            max depth      & 5, 20, None                        \\
            max features   & sqrt, log2, None                   \\
        \end{tabular}
    \end{center}
\end{table}

La table complète des résultats est en annexe \ref{grid_search_results}.

Le meilleur modèle obtient un f1 score de 0.13 avec les paramètres en table \ref{rf_params}.
Ce score est très faible, nous allons étudier les performances du modèle sur le set de validation pour comprendre ce résultat.

\begin{table}[H]
    \begin{center}
        \caption{\label{rf_params}Paramètres du meilleur modèle}
        \begin{tabular}{r|l}
            hyperparamètre & valeur             \\ \hline
            class weight   & balanced subsample \\
            n estimator    & 2000               \\
            max depth      & 5                  \\
            max features   & sqrt               \\
        \end{tabular}
    \end{center}
\end{table}

Les métriques des résultats sont observables en table \ref{rf_results} et la matrice de confusion en table \ref{rf_matrix}.
En raison de la nature déséquilibrée de notre classification, on ne peut pas se fier à la métrique d'accuracy.

\begin{table}[H]
    \begin{center}
        \caption{\label{rf_matrix}Matrice de confusion de l'entrainement du premier modèle, les colonnes représentent les prédictions et les lignes le ground truth.}
        \begin{tabular}{r|cc}
                    & Négatif & Positif \\ \hline
            Négatif & 10470   & 3422    \\
            Positif & 103     & 281     \\
        \end{tabular}
    \end{center}
\end{table}

\begin{table}[H]
    \begin{center}
        \caption{\label{rf_results}Matrice de confusion de l'entrainement du premier modèle, les colonnes représentent les prédictions et les lignes le ground truth.}
        \begin{tabular}{r|ccc}
            Classe  & Precision & Recall & F1Score \\ \hline
            Négatif & 0.99      & 0.75   & 0.86    \\
            Positif & 0.08      & 0.73   & 0.14    \\
        \end{tabular}
    \end{center}
\end{table}

On remarque dans les résultats que le modèle à de la peine à classifier les cas positifs avec beaucoup de faux-positifs.

\subsubsection{Analyse des résultats}
Premièrement, pour confirmer que le modèle utilise correctement les variables pour la décision nous étudions les raisons derrière ses décisions avec des méthodes d'explications.

Random Forest étant un algorithme interprétable, il nous permet de mesurer en figure \ref{feature_importance_rf_pit} l'importance des features dans sa classification.
L'importance d'une feature est calculée comme la réduction totale de l'entropie normalisée apportée par cette feature.
\fig[H, width=0.9\textwidth]{
    \label{feature_importance_rf_pit}Importance des features du modèle de Random Forest pour la classification des arrêts au stand.
}{feature_importance_rf_pit.svg}

Les features TyreLife, LapNumber et Stint sont les plus importantes. Ce qui est cohérent avec nos connaissances sur la stratégie de course.
Les features les moins utilisées sont celles liées au circuit, nous supposons que c'est parce que la feature est one-hot encoded se qui crée une grande matrice creuse
les features individuelles apporte donc peu d'information au modèle.

Pour comprendre les résultats, nous allons analyser les prédictions du modèle sur le set de validation.
Random Forest est un algorithme interprétable localement de lui même mais le grand nombre d'arbre dans notre modèle rend cela plus difficile.
Nous utilisons LIME (Local Interpretable Model-agnostic Explanations) \cite{lime} un outil d'explication locale,
c'est à dire qu'il permet d'expliquer les prédictions pour une instance donnée.
Pour ce faire LIME fonctionne de la manière suivante:
\begin{enumerate}
    \item On génère un ensemble de données autour de l'instance à expliquer en perturbant légerement les features.
    \item On prédit la classe de l'ensemble de données généré avec le modèle.
    \item On entraine un modèle explicable avec comme features l'ensemble de données généré et comme labels les prédictions du modèle. LIME attribue un poids à chaque instance de l'ensemble de données généré en fonction de la distance avec l'instance à expliquer. Les instances les plus proches sont plus importantes car elles sont plus représentatives de l'instance que l'on cherche à expliquer.
\end{enumerate}

Nous allons étudier les décisions prises pendant la course de Lance Stroll au Grand Prix d'Espagne 2023,
on peut observer la progression  des probabilités d'arrêt au stand en figure \ref{stroll_barcelona_2023}.
\fig[H, width=0.9\textwidth]{
    \label{stroll_barcelona_2023}Progression des probabilités d'arrêt au stand pour le Grand Prix d'Espagne 2023 de Lance Stroll.
    Les probabilités sont calculées à chaque tour en utilisant les données du tour précédent.
    Les lignes verticales rouges indiquent la target du modèle, c'est à dire un tour avant l'arrêt au stand réel.
    Les bares horizontales en haut de la figure indique le composé de pneu utilisé (rouge = tendre, jaune = médium, gris = dur).
}{barcelona_2023_lance_stroll.svg}

Le modèle semble capturer la relation entre les arrêts au stand et les données de course.
Les probabilités d'arrêt au stand augmentent à chaque tour et diminuent drastitquement après un arrêt au stand réel.
Les probabilités augmentent également plus faiblement pendant le relai sur pneus durs se qui est cohérent avec la dégradation plus faible de ces pneus.
Cependant le modèle ne semble pas pouvoir augmenter la probabilité d'arrêt au stand assez rapidement pour éviter les faux positifs, cela explique les résultats de la matrice de confusion.
Ces résultats sont cohérents avec les observations de l'étude \cite{app10217805}.

Nous allons maintenant étudier les explications de LIME pour les tours 2, 8 et 16 de cette course.
Premièrement, on peut observer en figure \ref{lime_2} les résultats de LIME pour le tour 2, où le modèle à décidé de ne pas s'arrêter.
\fig[H, width=0.9\textwidth]{
    \label{lime_2}Résultat de l'algorithme d'explication LIME pour le tour 2 de la course de Lance Stroll à Barcelone en 2023.
}{explanation_barcelona_stroll_2.svg}

On peut voir que l'age des pneus faible est la raison principale de la décision. En figure \ref{lime_8} les résultats de LIME pour le tour 8, le premier tour où le modèle décide de s'arrêter.
\fig[H, width=0.9\textwidth]{
    \label{lime_8}Résultat de l'algorithme d'explication LIME pour le tour 8 de la course de Lance Stroll à Barcelone en 2023.
}{explanation_barcelona_stroll_8.svg}

L'age des pneus a augmenté et sont impact négatif sur la décision est plus faible. L'absence de pneumatique dur et le fait que la voiture soit dans son
premier relai sont les raisons principales de la décision. En figure \ref{lime_16} les résultats de LIME pour le tour 16, le premier tour après l'arrêt au stand.
\fig[H, width=0.9\textwidth]{
    \label{lime_16}Résultat de l'algorithme d'explication LIME pour le tour 16 de la course de Lance Stroll à Barcelone en 2023.
}{explanation_barcelona_stroll_16.svg}
La prédiction est redevenue négative car l'age des pneus est de nouveau faible. De plus, on peut observer l'impact négatif sur la prédiction du second relai.
Le modèle a appris que les voitures font au moins deux relais il est donc normal que le second relai ait un impact négatif sur la prédiction.
Dans l'ensemble, les features utilisées par le modèle sont cohérentes avec nos connaissances sur la stratégie de course.

Pour étudier le comportement du modèle face au périodes de safety car, on observe la progression des probabilités d'arrêt au stand pour le Grand Prix d'Azerbaïdjan 2023 de Sergio Perez en figure \ref{baku_2023}.

\fig[H, width=0.9\textwidth]{
    \label{baku_2023}Progression des probabilités d'arrêt au stand pour le Grand Prix d'Azerbaïdjan 2023 de Sergio Perez.
    Les zones jaunes indiquent les périodes de drapeau jaune, de safety car ou de virtual safety car.
}{baku_2023_sergio_perez.svg}

La période de drapeau jaune au tour 10 a poussé le modèle à prédire un arrêt au stand pour Sergio Perez.
Ce qui est cohérent avec la stratégie de course de l'équipe Red Bull qui a effectué un arrêt au stand pour Sergio Perez au tour 11.
En figure \ref{lime_baku_10} on peut observer les résultats de LIME pour le tour 10, le drapeau jaune est la raison principale de la décision.
\fig[H, width=0.9\textwidth]{
    \label{lime_baku_10}Résultat de l'algorithme d'explication LIME pour le tour 10 de la course de Sergio Perez à Bakou en 2023.
}{explanation_baku_10.svg}

Cependant la période de drapeau jaune au tour 49 a également poussé le modèle à prédire un arrêt au stand alors qu'il n'y en a pas eu.
Cela est dû à un manque d'information sur la raison du drapeau jaune. La première période de drapeau jaune est due à un accident et donc est fortement suseptible d'entrainer une voiture de sécurité.
Alors que la deuxième période de drapeau jaune est due à une sortie de piste sans conséquence et donc n'entraine pas de voiture de sécurité.
Ces information ne sont pas disponibles dans les données car elles sont observables uniquement en regardant les images de la course.
En figure \ref{lime_baku_49} on peut voir que le drapeau jaune est encore une fois la cause de la décision.
\fig[H, width=0.9\textwidth]{
    \label{lime_baku_49}Résultat de l'algorithme d'explication LIME pour le tour 49 de la course de Sergio Perez à Bakou en 2023.
}{explanation_baku_49.svg}

\subsection{Réseaux de neurones}
Les réseaux de neurones sont des modèles plus puissant que Random Forest. Nous allons donc les tester sur le problème de prédiction d'arrêt au stand
pour voir si ils sont capables d'obtenir de meilleurs résultats. Pour ce faire nous avons utilisé la libraire Keras \cite{chollet2015keras} qui permet de construire des réseaux de neurones en Python.
Nous avons testés plusieurs architectures de réseaux de neurones et nous avons obtenu les meilleurs résultats avec l'architecture suivante \ref{neural_network_architecture} :
\begin{table}[H]
    \begin{center}
        \begin{tabular}{|c|c|c|}
            Couche             & Nombre de neurones & Fonction d'activation \\
            \hline
            Couche d'entrée    & 64                 & ReLU                  \\
            1ère couche cachée & 64                 & ReLU                  \\
            Dropout            & 0.2                & -                     \\
            2ème couche cachée & 64                 & ReLU                  \\
            Dropout            & 0.2                & -                     \\
            3ème couche cachée & 64                 & ReLU                  \\
            Dropout            & 0.2                & -                     \\
            Couche de sortie   & 1                  & Sigmoid               \\
        \end{tabular}
    \end{center}
    \caption{Architecture du réseau de neurones utilisé pour la prédiction d'arrêt au stand.}
    \label{neural_network_architecture}
\end{table}

Les couches Dropout sont des couches qui désactivent aléatoirement un certain pourcentage de neurones à chaque itération.
Le réseau de neurones est donc obligé de s'adapter à l'absence de certains neurones et ne peut pas se reposer sur un seul neurone.
Cela nous permet de réduire l'overfitting du modèle et d'améliorer sa capacité de généralisation.
Nous avons également utilisé la technique de Early Stopping qui permet d'arrêter l'entrainement du modèle si la performance sur le jeu de validation ne s'améliore plus.
Nous avons choisi une batch size assez grande de 256 pour s'asssurer que chaque batch contiennent des arrêts au stand.
Comme avec les Random Forest, nous avons réservé 20\% des données pour le jeu de validation et nous avons entrainé le modèle sur les 80\% restants.
Vous pouvez retrouver en table \ref{neural_network_hyperparameters} les hyperparamètres utilisés pour l'entrainement du modèle.
\begin{table}[H]
    \begin{center}
        \begin{tabular}{|c|c|}
            Hyperparamètre & Valeur              \\
            \hline
            Optimizer      & Nadam               \\
            Loss           & Binary Crossentropy \\
            Learning rate  & 0.001               \\
            Epochs         & 300                 \\
            Batch size     & 256                 \\
        \end{tabular}
    \end{center}
    \caption{Hyperparamètres utilisés pour l'entrainement du réseau de neurones.}
    \label{neural_network_hyperparameters}
\end{table}
Pour palier au problème de déséquilibre des classes, nous avons utilisé l'argument class\_weight de la fonction fit de Keras.
Cet argument permet de donner un poids à chaque classe pour le calcul de la loss. Les poids des classes ont étés calculés avec la méthode compute\_class\_weight \cite{classweight}
de la librairie scikit-learn. Les pondérations résultantes sont 0.5 pour la classe 0 et 17 pour la classe 1.

Nous avons obtenu les résultats suivants en table \ref{neural_network_results} avec la matrice de confusion en figure \ref{neural_network_confusion_matrix} :
\begin{table}[H]
    \begin{center}
        \caption{\label{neural_network_confusion_matrix}Matrice de confusion du réseau de neurones.}
        \begin{tabular}{r|cc}
                    & Négatif & Positif \\ \hline
            Négatif & 10053   & 2665    \\
            Positif & 121     & 247     \\
        \end{tabular}
    \end{center}
\end{table}

\begin{table}[H]
    \begin{center}
        \caption{\label{neural_network_results}Métriques de performance du réseau de neurones.}
        \begin{tabular}{r|ccc}
            Classe  & Precision & Recall & F1Score \\ \hline
            Négatif & 0.99      & 0.79   & 0.88    \\
            Positif & 0.08      & 0.67   & 0.15    \\
        \end{tabular}
    \end{center}
\end{table}
Les résultats sont très similaires à ceux obtenus avec les Random Forest. Mais en étudiant le comportement du modèle sur les données de validation,
nous avons remarqué un meilleur comportement. En figure \ref{miami_2023_sainz} vous pouvez voir la différence de comportement entre les deux modèles pour la course de Carlos Sainz sur le Grand Prix de Miami en 2023.
\fig[H, width=0.9\textwidth]{
    \label{miami_2023_sainz}Prédiction d'arrêt au stand pour la course de Carlos Sainz sur le Grand Prix de Miami en 2023.
}{miami_2023_sainz.svg}
Ici le modèle de Random Forest commence à prédire des arrêt au stand dès le 8ème tour, alors que le modèle de réseau de neurones ne commence à prédire des arrêts au stand qu'à partir du 14ème.
Le modèle de réseau de neurones semble pouvoir augmenter sa probabilité d'arrêt au stand plus rapidement que le modèle de Random Forest ce qui est un comportement plus utile pour la stratégie de course.
En étudiant les résultats de LIME sur le 8ème tour de la course en figure \ref{miami_2023_sainz_lime_rf} et \ref{miami_2023_sainz_lime_nn},
on peut voir que le réseau de neurones accordre plus d'importance à l'age des pneus -0.3 contre -0.15 pour le modèle de Random Forest.
\fig[H, width=0.9\textwidth]{
    \label{miami_2023_sainz_lime_rf}Résultats de LIME pour le modèle de Random Forest sur le 8ème tour de la course de Carlos Sainz sur le Grand Prix de Miami en 2023.
}{miami_2023_sainz_lime_rf.svg}
\fig[H, width=0.9\textwidth]{
    \label{miami_2023_sainz_lime_nn}Résultats de LIME pour le modèle de réseau de neurones sur le 8ème tour de la course de Carlos Sainz sur le Grand Prix de Miami en 2023.
}{miami_2023_sainz_lime_nn.svg}
Malgré les métriqes de performance similaires, le modèle de réseau de neurones semble avoir un meilleur comportement.

\subsection{Séries temporelles}
Les précédentes expériences utilisaient uniquement le dernier tour pour prédire la probabilité d'arrêt au stand.
Cependant, il sera plus intéressant d'utiliser les données de plusieurs tours car cela permettra de mieux capturer les tendances.

La formulation du problème devient alors une prédiction de séries temporelles.
On considère alors les données des tours comme une séquence.
Cette formulation est plus naturelle car les données sont une discrétisation par tour de données de capteurs qui sont des séries temporelles.
Elles sont enregistrées de façon séquentielle et sont donc naturellement ordonnées et corrélées.

Il faut cependant grouper les données par course et par voiture pour avoir des séquences de données cohérentes.
On peut observer en figure \ref{time_series_example} la nature séquentielle des données pour un sous ensemble de features.
\fig[H, width=0.9\textwidth]{
    \label{time_series_example}Exemple de séries temporelles pour la course de Lewis Hamilton sur le Grand Prix d'Espagne en 2023.
}{time_series_example.svg}

L'étape de préparation des données est plus complexe que pour les modèles précédents.
Il faut maintenant créer des séquences de données de taille fixe à partir des données de chaque course.
Les données des course de chaque voiture doivent être dans le même set de données (entrainement ou validation).
Le processus de séparation des données est le suivant :
\begin{enumerate}
    \item On regroupe les données avec les colonnes \textit{Year}, \textit{RoundNumber} et \textit{DriverNumber} comme clé.
    \item On mélange ces clés puis on les sépare en 80\% pour l'entrainement, 20\% pour la validation.
    \item On crée les 2 sets de données en utilisant les clés séparées.
\end{enumerate}
La séparation n'est plus exactement fidèle au pourcentage spécifié car on sépare les courses et non les tours.
Mais avec notre grand nombre de données cela ne pose pas de problème.
Ensuite, on crée les séquences de données de taille fixe à partir des données de chaque course.
La cible de chaque séquence est la colonne "is\_pitting" du prochain tour,
par exemple, la cibe d'une séquence comprenant les tours de 10 à 15 est la valeur de "is\_pitting" du tour 16.

Pour cette modélisation nous avons utilisé un réseau de neurones récurrents (RNN) avec des cellules LSTM.
Les réseau de neurones récurrents sont un type de réseau de neurones adapté aux séries temporelles.
Les connexions entre les neurones forment des cycles \ref{rnn_example}, ce qui permet d'utiliser les sorties précédentes comme entrées.
Cela permet aux informations passées d'avoir un impact sur les prédictions futures.
\fig[H, width=0.9\textwidth]{
    \label{rnn_example}Exemple de réseau de neurones récurrents.\cite{rnnExample}
}{rnn_example.svg}
Une des limitations des réseaux de neurones récurrents est le "vanishing gradient problem" où les gradients deviennent trop petits pour être utiles.
Les LSTM (Long Short Term Memory) sont une variante des RNN qui permettent de résoudre ce problème.
L'architecture utilisée est en table \ref{lstm_architecture}.
\begin{table}[H]
    \begin{center}
        \caption{\label{lstm_architecture}Architecture du réseau de neurones récurrents.}
        \begin{tabular}{| l | l | l |}
            Couche  & Nombre de neurones & Fonction d'activation \\ \hline
            LSTM    & 64                 & Tanh                  \\
            Dropout & 0.1                &                       \\
            LSTM    & 64                 & Tanh                  \\
            Dropout & 0.1                &                       \\
            Dense   & 64                 & ReLu                  \\
            Dropout & 0.1                &                       \\
            Dense   & 64                 & ReLu                  \\
            Dense   & 1                  & Sigmoid               \\
        \end{tabular}
    \end{center}
\end{table}

Vous pouvez retrouver en table \ref{rnn_hyperparameters} les hyperparamètres utilisés pour l'entrainement du modèle.
Contrairement au réseaux de neurones classiques nous avons observés de meilleures résultats avec un learning rate plus faible et un batch size plus grand.
\begin{table}[H]
    \begin{center}
        \begin{tabular}{|c|c|}
            Hyperparamètre & Valeur              \\
            \hline
            Optimizer      & Nadam               \\
            Loss           & Binary Crossentropy \\
            Learning rate  & 0.0005              \\
            Epochs         & 100                 \\
            Batch size     & 1024                \\
        \end{tabular}
    \end{center}
    \caption{Hyperparamètres utilisés pour l'entrainement du réseau de neurones.}
    \label{rnn_hyperparameters}
\end{table}

On peut voir en table \ref{rnn_matrix} et \ref{rnn_results} les résultats de ce modèle.
\begin{table}[H]
    \begin{center}
        \caption{\label{rnn_matrix}Matrice de confusion du réseau de neurones récurrents.}
        \begin{tabular}{r|cc}
                    & Négatif & Positif \\ \hline
            Négatif & 8914    & 2607    \\
            Positif & 91      & 242     \\
        \end{tabular}
    \end{center}
\end{table}

\begin{table}[H]
    \begin{center}
        \caption{\label{rnn_results}Métriques de performance du réseau de neurones récurrents.}
        \begin{tabular}{r|ccc}
            Classe  & Precision & Recall & F1Score \\ \hline
            Négatif & 0.99      & 0.77   & 0.87    \\
            Positif & 0.08      & 0.73   & 0.15    \\
        \end{tabular}
    \end{center}
\end{table}

On peut observer une très légère amélioration par rapport au modèle précédent. Cependant, le modèle est toujours très biaisé vers la classe négative.
En figure \ref{comparison_rnn} on peut voir la comparaison des modèles sur les données du Grand Prix d'Espagne 2023 de Max Verstappen.
\fig[H, width=0.9\textwidth]{
    \label{comparison_rnn}Comparaison des modèles sur les données du Grand Prix d'Espagne 2023 de Max Verstappen.
}{comparison_rnn.svg} %TODO fix the figure

On peut observer que le modèle récurrent a un comportement similaire au modèle précédent mais avec des prédictions plus confiantes.
Cependant, il semble sujet a des prédictions assez étrange comme en figure %\ref{rnn_prediction}.
% Add weird prediction figure

\section{Classification multi-classes pour le choix des pneus}
Dans cette section nous allons nous intéresser à la classification multi-classes pour le choix des pneus.
Elle serait utilisée en combinaison avec la classification binaire pour la prédiction des arrêts aux stands.
Le but est que une fois une précition positive pour un arrêt aux stands, on puisse prédire le type de pneus qui sera utilisé.

La préparation des données est similaires à la classification d'arrêt aux stands à la différence que la cible est la colonne "NextCompound".
De plus on ne garde que les tours où un arrêt aux stands a eu lieu. On réduit grandement le nombre d'échantillons (environ de 69'400 à 1'900)
mais le problème devient plus simple car les classes sont plus équilibrées, comme on peut le voir en figure \ref{Compound_distribution}.

Comme avec la classification binaire, nous avons expérimenté avec des modèles Random Forest et des réseaux de neurones.
Le modèle Random Forest a été entrainé avec les hyperparamètres en table \ref{rf_hyperparameters_compound}.
\begin{table}[H]
    \begin{center}
        \caption{\label{rf_hyperparameters_compound}Paramètres du meilleur modèle Random Forest pour le choix des pneus.}
        \begin{tabular}{r|l}
            hyperparamètre & valeur             \\ \hline
            class weight   & balanced subsample \\
            n estimator    & 2000               \\
            max depth      & 5                  \\
            max features   & sqrt               \\
        \end{tabular}
    \end{center}
\end{table}
%TODO faire un grid search pour chercher un meilleur modèle

On peut voir en table \ref{rf_compound_matrix} et \ref{rf_compound_results} les résultats de ce modèle.
\begin{table}[H]
    \begin{center}
        \caption{\label{rf_compound_matrix}Matrice de confusion du meilleur modèle Random Forest pour le choix des pneus.
            Les colonnes représentent les prédictions et les lignes les vraies valeurs.}
        \begin{tabular}{r|ccc}
                   & Soft & Medium & Hard \\ \hline
            Soft   & 83   & 7      & 1    \\
            Medium & 59   & 63     & 7    \\
            Hard   & 68   & 27     & 69   \\
        \end{tabular}
    \end{center}
\end{table}
\begin{table}[H]
    \begin{center}
        \caption{\label{rf_compound_results}Métriques de performance du meilleur modèle Random Forest pour le choix des pneus.}
        \begin{tabular}{r|ccc}
            Classe & Precision & Recall & F1Score \\ \hline
            Soft   & 0.71      & 0.70   & 0.70    \\
            Medium & 0.65      & 0.49   & 0.56    \\
            Hard   & 0.9       & 0.42   & 0.57    \\
        \end{tabular}
    \end{center}
\end{table}
Les résultats sont assez particulier, le modèle prédit souvent la classe Soft, on a beaucoup de faux positifs pour cette classe.
La classe Hard est rarement prédite mais quand elle l'est, elle est souvent correcte.
En figure \ref{rf_compound_prediction} on peut voir la comparaison des prédictions du modèle avec les vraies valeurs.
\fig[H, width=0.9\textwidth]{
    \label{rf_compound_prediction}Prédiction du modèle de Random Forest pour le choix des pneus du grand prix d'Espagne 2023 de Max Verstappen.
}{rf_compound_prediction.svg}
En début de course, le modèle prédit la classe Hard se qui fait sens car c'est le pneu le plus résistant sur lequel on serait le plus susceptible de faire un long relais jusqu'à la fin de la course.
Puis en milieu de course le modèle choisit des pneus Medium avant de choisir des pneus Soft à la fin de la course.
Cela semble être un comportement logique pour un pilote qui cherche à faire le moins d'arrêts aux stands possible.

\section{Autre modélisation}
Nous avons expérimentés les autres modélisation dans lesquelles les deux problèmes sont traités en même temps.
En théorie cela permettrait d'avoir un modèle plus performant car les décisions sont liées.
Deux pistes explorées sont celle d'une classification avec 4 classes ou les 2 colonnes "is\_pitting" et "NextCompound" sont combinées en une seule cible
codifiée de la manière suivante :
\begin{table}[H]
    \begin{center}
        \begin{tabular}{r|c}
            Classe                             & Valeur       \\ \hline
            Pas d'arrêt aux stands             & [1, 0, 0, 0] \\
            Arrêt aux stands avec pneus Soft   & [0, 1, 0, 0] \\
            Arrêt aux stands avec pneus Medium & [0, 0, 1, 0] \\
            Arrêt aux stands avec pneus Hard   & [0, 0, 0, 1] \\
        \end{tabular}
    \end{center}
\end{table}

L'autre piste est celle d'un réseau de neurones avec 2 sorties, une pour chaque problème.
L'une des sorties est une classification binaire pour la prédiction des arrêts aux stands soit un seul neurone avec une fonction d'activation sigmoïde.
L'autre sortie est une classification multi-classes pour le choix des pneus soit 3 neurones avec une fonction d'activation softmax.
Chacune des sorties possède sa propre fonction de perte. Une telle architecture est illustrée en figure \ref{two_output_nn}.
\fig[H, width=0.9\textwidth]{
    \label{two_output_nn}Architecture d'un réseau de neurones avec 2 sorties.
}{two_output_nn.svg}

Ces deux pistes n'ont pas donné de résultats concluants. Les modèles sont bien moins performants que ceux présentés dans les sections précédentes.
Cela peut être dû au fait que la sur-représentation de la classe "Pas d'arrêt aux stands" rend difficile l'apprentissage des classes du choix des pneus.

\section{Métrique de performance}
Suite à l'analyse des résultats des différents modèles explorés dans ce chapitre, on a pu observer que
les métriques qui considérait les prédictions comme des résultats booléens (True positif, False positif, ...) étaient assez peu représentatives de la performance des modèles.

En effet, dans son utilisation finale, le modèle n'as pas besoin d'être restraint à une prise de décision binaire avec des seuils de probabilité.
Il peut simplement donner une probabilité d'arrêt aux stands pour chaque tour de la course. C'est alors aux ingénieurs de l'équipe de décider
si ce seuil est suffisamment élevé pour justifier un arrêt aux stands.

Il fait donc plus sens d'utiliser des métriques qui considèrent les prédictions comme des probabilités et qui sont calculées sur l'ensemble des prédictions d'une course,
le scénario d'utilisation du modèle final. L'objectif est de faire la différence entre un modèle ne prédirait pas du tout un arrêt aux stands et un modèle qui l'aurait prédit
avec quelques tours de retard.

La métrique proposée pondère les prédictions d'une course en fonction de leur impact. Les tours où la voiture rentre aux stands, les modèles sont récompensés
proportionnellement à la probabilité d'arrêt prédite.